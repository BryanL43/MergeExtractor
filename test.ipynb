{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 3802588 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 119\u001b[0m\n\u001b[0;32m    105\u001b[0m startPhrases \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground of the transaction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground of the merger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground of the offer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground of the acquisition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m ];\n\u001b[0;32m    112\u001b[0m stopPhrases \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasons for the Transactions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasons for the merger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasons for the offer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasons for the acquisition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m ];\n\u001b[1;32m--> 119\u001b[0m backgroundSection \u001b[38;5;241m=\u001b[39m \u001b[43mextractSection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncatedText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartPhrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopPhrases\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(backgroundSection);\n",
      "Cell \u001b[1;32mIn[2], line 75\u001b[0m, in \u001b[0;36mextractSection\u001b[1;34m(text, startCandidates, stopCandidates)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextractSection\u001b[39m(text, startCandidates, stopCandidates):\n\u001b[1;32m---> 75\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Initialize start and stop indexes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\envs\\MergeExtractor\\Lib\\site-packages\\spacy\\language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\envs\\MergeExtractor\\Lib\\site-packages\\spacy\\language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\envs\\MergeExtractor\\Lib\\site-packages\\spacy\\language.py:1120\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \n\u001b[0;32m   1116\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1121\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 3802588 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def loadFile(filePath):\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read();\n",
    "\n",
    "# Parse the HTML content, extract text from the parsed HTML, & do some initial clean-up\n",
    "def preProcessText(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\");\n",
    "    text = soup.get_text(separator=\"\");\n",
    "\n",
    "    # Remove standalone page numbers\n",
    "    pageNumPattern = re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE);\n",
    "    text = re.sub(pageNumPattern, '', text);\n",
    "\n",
    "    # Remove extra newline characters\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text);\n",
    "\n",
    "    return text.strip();\n",
    "\n",
    "def removeTableOfContents(text):\n",
    "    # Regular expression patterns for table of contents\n",
    "    tocStartPattern = re.compile(r'(Table of Contents|Contents|TABLE OF CONTENT|CONTENTS)', re.IGNORECASE);\n",
    "    tocEndPattern = re.compile(r'(Introduction|Chapter \\d+|Section \\d+|Part \\d+|Page \\d+)', re.IGNORECASE);\n",
    "\n",
    "    # Find the start of the table of contents\n",
    "    tocStartMatch = tocStartPattern.search(text);\n",
    "    if not tocStartMatch: # No table of contents found\n",
    "        return text;\n",
    "\n",
    "    tocStartIndex = tocStartMatch.start();\n",
    "\n",
    "    # Find the end of the table of contents\n",
    "    tocEndMatch = tocEndPattern.search(text, tocStartIndex);\n",
    "    if not tocEndMatch: # No end of table of contents found\n",
    "        return text;\n",
    "\n",
    "    tocEndIndex = tocEndMatch.start();\n",
    "\n",
    "    # Remove the table of contents section\n",
    "    cleanedText = text[:tocStartIndex] + text[tocEndIndex:];\n",
    "\n",
    "    # Remove any remaining table of contents references\n",
    "    cleanedText = re.sub(r'\\btable\\s*of\\s*contents?\\b|\\btableofcontents?\\b', '', cleanedText, flags=re.IGNORECASE);\n",
    "\n",
    "    # Handle concatenated cases\n",
    "    cleanedText = re.sub(r'(?i)table\\s*of\\s*contents?|tableofcontents?', '', cleanedText)\n",
    "\n",
    "    return cleanedText.strip();\n",
    "\n",
    "def extractSection(text, startCandidates, stopCandidates):\n",
    "    # Tokenize text into lines\n",
    "    lines = text.split(\"\\n\");\n",
    "    startIndex, stopIndex = -1, -1;\n",
    "\n",
    "    # Identify the start and stop indexes using fuzzy matching\n",
    "    for i, line in enumerate(lines):\n",
    "        if startIndex == -1 and any(fuzz.partial_ratio(line.lower(), sc.lower()) > 95 for sc in startCandidates):\n",
    "            startIndex = i;\n",
    "        if startIndex != -1 and i > startIndex and any(fuzz.partial_ratio(line.lower(), sc.lower()) > 80 for sc in stopCandidates):\n",
    "            stopIndex = i;\n",
    "            break;\n",
    "\n",
    "    # Extract the section\n",
    "    if startIndex != -1 and stopIndex != -1 and startIndex != stopIndex:\n",
    "        return \"\\n\".join(lines[startIndex:stopIndex]).strip();\n",
    "    return None;\n",
    "\n",
    "text = loadFile(\"./SampleData/sample4.txt\");\n",
    "cleanedText = preProcessText(text);\n",
    "cleanedText = removeTableOfContents(cleanedText);\n",
    "truncatedText = cleanedText[50000:1000000]; # Chop off the table of contents\n",
    "\n",
    "startPhrases = [\n",
    "    \"Background of the transaction\",\n",
    "    \"Background of the merger\",\n",
    "    \"Background of the offer\",\n",
    "    \"background of the acquisition\"\n",
    "];\n",
    "\n",
    "stopPhrases = [\n",
    "    \"Reasons for the Transactions\",\n",
    "    \"Reasons for the merger\",\n",
    "    \"Reasons for the offer\",\n",
    "    \"Reasons for the acquisition\"\n",
    "];\n",
    "\n",
    "backgroundSection = extractSection(truncatedText, startPhrases, stopPhrases);\n",
    "print(backgroundSection);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MergeExtractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
