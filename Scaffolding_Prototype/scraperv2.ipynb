{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Event\n",
    "import re\n",
    "from tqdm import tqdm # Progress bar for visualizing data cleaning progress\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "# Load spaCy model\n",
    "if torch.cuda.is_available():\n",
    "    spacy.prefer_gpu();\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\");\n",
    "\n",
    "# Global termination flag\n",
    "terminationEvent = Event();\n",
    "\n",
    "# Defined global variables\n",
    "csvFile = \"../truncatedData.csv\";\n",
    "minDate = datetime(2001, 1, 1);\n",
    "formTypes = [\"PREM14A\", \"S-4\", \"SC 14D9\", \"SC TO-T\"];\n",
    "# mainIndex = 2;\n",
    "maxNumOfThreads = os.cpu_count(); # Assuming Window system\n",
    "\n",
    "# Read the CSV file and extract the date & both merging companies (index base)\n",
    "filedDate = pd.read_csv(csvFile, header=None).iloc[:, 1].tolist();\n",
    "companyAList = pd.read_csv(csvFile, header=None).iloc[:, 2].tolist();\n",
    "companyBList = pd.read_csv(csvFile, header=None).iloc[:, 3].tolist();\n",
    "\n",
    "# Phrases for locating start/end point of the background section\n",
    "startPhrases = [\n",
    "    \"Background of the transaction\",\n",
    "    \"Background of the merger\",\n",
    "    \"Background of the offer\",\n",
    "    \"Background of the acquisition\",\n",
    "    \"Background of the Offer and the Merger\"\n",
    "]\n",
    "\n",
    "stopPhrases = [\n",
    "    \"Reasons for the Transactions\",\n",
    "    \"Reasons for the merger\",\n",
    "    \"Reasons for the offer\",\n",
    "    \"Reasons for the acquisition\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire the constraint of a given date.\n",
    "# Pad 2 months backward and forward for constraint.\n",
    "def getDateConstraints(date):\n",
    "    originalDate = datetime.strptime(date, \"%m/%d/%Y\");\n",
    "\n",
    "    # Define the lower-bound date\n",
    "    lbMonth = originalDate.month - 2;\n",
    "    if (lbMonth <= 0): # Case: Wrap to previous year\n",
    "        lbMonth += 12;\n",
    "        lbYear = originalDate.year - 1;\n",
    "    else: # Case: Still on current year\n",
    "        lbYear = originalDate.year;\n",
    "\n",
    "    # Construct lower-bound date\n",
    "    try:\n",
    "        lowerBoundDate = originalDate.replace(year=lbYear, month=lbMonth);\n",
    "    except ValueError: # Catch potential error i.e. feb. 30 not existing\n",
    "        lowerBoundDate = originalDate.replace(year=lbYear, month=lbMonth, day=1);\n",
    "\n",
    "    # Ensure the new date does not go below the minimum date\n",
    "    if (lowerBoundDate < minDate):\n",
    "        lowerBoundDate = minDate;\n",
    "\n",
    "    \n",
    "    # Define the upper-bound date\n",
    "    ubMonth = originalDate.month + 2;\n",
    "    if (ubMonth > 12): # Case: Wrap to next year\n",
    "        ubMonth -= 12;\n",
    "        ubYear = originalDate.year + 1;\n",
    "    else: # Case: Still on current year\n",
    "        ubYear = originalDate.year;\n",
    "\n",
    "    # Construct upper-bound date\n",
    "    try:\n",
    "        upperBoundDate = originalDate.replace(year=ubYear, month=ubMonth);\n",
    "    except ValueError: # Catch potential error i.e. feb. 30 not existing\n",
    "        upperBoundDate = originalDate.replace(year=ubYear, month=ubMonth + 1, day=1);\n",
    "\n",
    "    return [lowerBoundDate, upperBoundDate];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of CIKs for the merging companies\n",
    "def getCIKS(searchCompany, pairCompany, dateLB, dateUB, formTypes):\n",
    "    restructName = searchCompany.replace(\" \", \"%20\");\n",
    "    \n",
    "    url = f\"https://efts.sec.gov/LATEST/search-index?q={restructName}&dateRange=custom&category=custom&startdt={dateLB}&enddt={dateUB}&forms={formTypes}\";\n",
    "\n",
    "    # Create a request that mimics browser activity\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.sec.gov/\"\n",
    "    }\n",
    "\n",
    "    # Request the search query & acquire the DOM elements\n",
    "    response = requests.get(url, headers=headers);\n",
    "    if (response.status_code != 200):\n",
    "        print(\"FATAL: getDocumentJson response yielded an error!\");\n",
    "        sys.exit(response.status_code);\n",
    "    \n",
    "    data = response.json();\n",
    "    totalValue = data[\"hits\"][\"total\"][\"value\"];\n",
    "\n",
    "    if (totalValue <= 0):\n",
    "        return None;\n",
    "    \n",
    "    # Formulate the list of entities for CIK lookup\n",
    "    entityList = [];\n",
    "    for entities in data[\"aggregations\"][\"entity_filter\"][\"buckets\"]:\n",
    "        entityList.append(entities[\"key\"]);\n",
    "\n",
    "    # Acquire the CIK for the given company using combined fuzzy matching techniques\n",
    "    threshold = 90\n",
    "    filteredMatch = [\n",
    "        entity for entity in entityList if fuzz.partial_ratio(pairCompany.lower(), entity.lower()) > threshold\n",
    "    ];\n",
    "    \n",
    "    # Extract the CIK from the filtered match\n",
    "    cikList = [];\n",
    "    for entity in filteredMatch:\n",
    "        cikList.append(re.search(r'\\(CIK (\\d+)\\)', entity).group(1));\n",
    "\n",
    "    return cikList if cikList else None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire all the json documents for the given companies with CIK filter\n",
    "def getCIKDocumentJson(searchCompany, pairCompany, dateLB, dateUB, formTypes):\n",
    "    # Remove parantheses content from the company names\n",
    "    searchCompany = re.sub(r'\\(.*\\)', '', searchCompany).strip();\n",
    "    pairCompany = re.sub(r'\\(.*\\)', '', pairCompany).strip();\n",
    "\n",
    "    # We will try and acquire the cikList for the first company;\n",
    "    # If the cikList is None, we will try and acquire the cikList for the second company.\n",
    "    cikList = getCIKS(searchCompany, pairCompany, dateLB, dateUB, formTypes);\n",
    "    if (cikList == None):\n",
    "        cikList = getCIKS(pairCompany, searchCompany, dateLB, dateUB, formTypes);\n",
    "    \n",
    "    if (cikList == None):\n",
    "        return None;\n",
    "\n",
    "    \"\"\"\n",
    "        - Fetch data for each CIK concurrently\n",
    "        - We do not need to verify if the hit returns nothing as if the entity is not found,\n",
    "            the CIK will not be present in the list.\n",
    "    \"\"\"\n",
    "    restructName = searchCompany.replace(\" \", \"%20\");\n",
    "\n",
    "    urls = [f\"https://efts.sec.gov/LATEST/search-index?q={restructName}&dateRange=custom&category=custom&startdt={dateLB}&enddt={dateUB}&forms={formTypes}&filter_ciks={cik}\" for cik in cikList];\n",
    "    \n",
    "    # Create a request that mimics browser activity\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.sec.gov/\"\n",
    "    }\n",
    "\n",
    "    # Fetch the json data for each CIK\n",
    "    if len(urls) == 1: # Case: Single URL; no threads required\n",
    "        response = requests.get(urls[0], headers=headers);\n",
    "        if (response.status_code != 200):\n",
    "            print(\"FATAL: getDocumentJson response yielded an error!\");\n",
    "            sys.exit(response.status_code);\n",
    "        \n",
    "        result = response.json();\n",
    "        mergedHits = result[\"hits\"][\"hits\"] if result and \"hits\" in result and \"hits\" in result[\"hits\"] else [];\n",
    "    else: # Case: Multiple URLs; use threads for concurrent fetching\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(lambda url: requests.get(url, headers=headers), urls));\n",
    "        \n",
    "        # Merge the results into a single list\n",
    "        mergedHits = [];\n",
    "        for response in results:\n",
    "            if (response.status_code != 200):\n",
    "                print(\"FATAL: getDocumentJson response yielded an error!\");\n",
    "                sys.exit(response.status_code);\n",
    "\n",
    "            result = response.json();\n",
    "            if result and \"hits\" in result and \"hits\" in result[\"hits\"]:\n",
    "                mergedHits.extend(result[\"hits\"][\"hits\"]);\n",
    "\n",
    "    return mergedHits if mergedHits else None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - No documents were found associated with the CIKs.\n",
    "    - We will let fuzzy match determine if the company is present in the document.\n",
    "        - Return a truncated list of documents for both companies without the cik filter.\n",
    "    - Basically throwing a dart at the board and hoping it hits the target if no cik filtering is found.\n",
    "\"\"\"\n",
    "def getDocumentJson(searchCompany, pairCompany, dateLB, dateUB, formTypes):\n",
    "    # Remove parantheses content from the company names\n",
    "    searchCompany = re.sub(r'\\(.*\\)', '', searchCompany).strip();\n",
    "    pairCompany = re.sub(r'\\(.*\\)', '', pairCompany).strip();\n",
    "\n",
    "    restructSearch = searchCompany.replace(\" \", \"%20\");\n",
    "    restructPair = pairCompany.replace(\" \", \"%20\");\n",
    "\n",
    "    urls = [\n",
    "        f\"https://efts.sec.gov/LATEST/search-index?q={restructSearch}&dateRange=custom&category=custom&startdt={dateLB}&enddt={dateUB}&forms={formTypes}\",\n",
    "        f\"https://efts.sec.gov/LATEST/search-index?q={restructPair}&dateRange=custom&category=custom&startdt={dateLB}&enddt={dateUB}&forms={formTypes}\"\n",
    "    ];\n",
    "    \n",
    "    # Create a request that mimics browser activity\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.sec.gov/\"\n",
    "    }\n",
    "\n",
    "    # Fetch the json data for each company using threads for concurrent fetching\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda url: requests.get(url, headers=headers), urls));\n",
    "    \n",
    "    # Merge the results into a single list\n",
    "    mergedHits = [];\n",
    "    for response in results:\n",
    "        if (response.status_code != 200):\n",
    "            print(\"FATAL: getDocumentJson response yielded an error!\");\n",
    "            sys.exit(response.status_code);\n",
    "\n",
    "        result = response.json();\n",
    "        if result and \"hits\" in result and \"hits\" in result[\"hits\"]:\n",
    "            mergedHits.extend(result[\"hits\"][\"hits\"]);\n",
    "\n",
    "    return mergedHits if mergedHits else None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate the source document links from the search result json\n",
    "def getSourceLinks(documentJson):\n",
    "    # Formulate all source document file links\n",
    "    sourceLinks = [];\n",
    "\n",
    "    # Iterate through each json object and construct the source document file links\n",
    "    for document in documentJson:\n",
    "        try:\n",
    "            # Get the CIK id or if there is multiple, then acquire the last one\n",
    "            validatedCik = None;\n",
    "            ciks = document[\"_source\"][\"ciks\"];\n",
    "            if ciks:\n",
    "                lastCIK = ciks[-1];\n",
    "                validatedCik = lastCIK;\n",
    "\n",
    "            # Remove leading zeros from the CIK\n",
    "            validatedCik.lstrip('0');\n",
    "        \n",
    "            # Acquire normal adsh & adsh without the \"-\" character\n",
    "            adsh = document[\"_source\"][\"adsh\"];\n",
    "            truncatedADSH = document[\"_source\"][\"adsh\"].replace(\"-\", \"\");\n",
    "            \n",
    "            sourceLinks.append(f\"https://www.sec.gov/Archives/edgar/data/{validatedCik}/{truncatedADSH}/{adsh}.txt\");\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in document: {e}, result: {document}\");\n",
    "            continue; # Skip the document if there is a missing key; logged for further investigation\n",
    "\n",
    "    return sourceLinks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFileFromURL(url):\n",
    "    # Create a request that mimics browser activity\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers);\n",
    "    if (response.status_code == 200):\n",
    "        return response.text;\n",
    "    else:\n",
    "        print(\"FATAL: Failed to load document via url.\");\n",
    "        sys.exit(response.status_code);\n",
    "\n",
    "def preProcessText(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\");\n",
    "    text = soup.get_text(separator=\"\\n\");\n",
    "\n",
    "    # Remove standalone page numbers\n",
    "    pageNumPattern = re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE);\n",
    "    text = re.sub(pageNumPattern, '', text);\n",
    "\n",
    "    # Remove extra newline characters\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text);\n",
    "\n",
    "    return text.strip();\n",
    "\n",
    "def extractFirstWord(companyName):\n",
    "    clean_name = re.sub(r\"\\(.*?\\)\", \"\", companyName);  # Remove parentheses content\n",
    "    return clean_name.split()[0];\n",
    "\n",
    "def checkCompaniesInDocument(url, companyNames):\n",
    "    if (terminationEvent.is_set()):\n",
    "        return None, False  # Exit early if thread termination is triggered\n",
    "    \n",
    "    rawText = loadFileFromURL(url);\n",
    "    if (not rawText):  # If we cannot load the document\n",
    "        return \"\", False;\n",
    "\n",
    "    cleanedText = preProcessText(rawText);\n",
    "    lowerText = cleanedText.lower();\n",
    "\n",
    "    # Check if both company names are present as whole words\n",
    "    foundCompanies = [name for name in companyNames if re.search(r'\\b' + re.escape(name) + r'\\b', lowerText)];\n",
    "    \n",
    "    # Return the cleanedText if both company names are found, else False\n",
    "    return cleanedText, len(foundCompanies) == len(companyNames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTableOfContents(text):\n",
    "    # Regular expression patterns for table of contents\n",
    "    tocStartPattern = re.compile(r'(Table of Contents|Contents|TABLE OF CONTENT|CONTENTS)', re.IGNORECASE);\n",
    "    tocEndPattern = re.compile(r'(Introduction|Chapter \\d+|Section \\d+|Part \\d+|Page \\d+)', re.IGNORECASE);\n",
    "\n",
    "    # Find the start of the table of contents\n",
    "    tocStartMatch = tocStartPattern.search(text);\n",
    "    if (not tocStartMatch):  # No table of contents found\n",
    "        return text;\n",
    "\n",
    "    tocStartIndex = tocStartMatch.start();\n",
    "\n",
    "    # Find the end of the table of contents\n",
    "    tocEndMatch = tocEndPattern.search(text, tocStartIndex);\n",
    "    if (not tocEndMatch):  # No end of table of contents found\n",
    "        return text;\n",
    "\n",
    "    tocEndIndex = tocEndMatch.start();\n",
    "\n",
    "    # Remove the table of contents section\n",
    "    cleanedText = text[:tocStartIndex] + text[tocEndIndex:];\n",
    "\n",
    "    # Remove any remaining table of contents references\n",
    "    cleanedText = re.sub(r'\\btable\\s*of\\s*contents?\\b|\\btableofcontents?\\b', '', cleanedText, flags=re.IGNORECASE);\n",
    "    cleanedText = re.sub(r'(?i)table\\s*of\\s*contents?|tableofcontents?', '', cleanedText);\n",
    "\n",
    "    return cleanedText.strip();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSection(text, startCandidates):\n",
    "    doc = nlp(text);\n",
    "    sentences = [sent.text for sent in doc.sents];\n",
    "\n",
    "    startIndex = -1;\n",
    "\n",
    "    # Locate the start of the desired background section\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        match = next(\n",
    "            (sc for sc in startCandidates if sc.lower() in sentence.lower() or fuzz.partial_ratio(sentence.lower(), sc.lower()) > 95),\n",
    "            None\n",
    "        );\n",
    "        if match:\n",
    "            startIndex = i;\n",
    "            break;\n",
    "    \n",
    "    # No \"Background\" section found\n",
    "    if (startIndex == -1 or match is None):\n",
    "        return None;\n",
    "    \n",
    "    section = sentences[startIndex:];\n",
    "\n",
    "    return \"\\n\".join(section);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_timestamp():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\");\n",
    "\n",
    "def logMessage(message):\n",
    "    with open(f\"../logs.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(message + \"\\n\");\n",
    "\n",
    "    print(message);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index:  11 ; Companies:  Dallas-Semiconductor Corp  &  Maxim Integrated Products Inc\n",
      "False\n",
      "False\n",
      "[2025-01-30 19:52:03] [-] No background section found for index 11: Dallas-Semiconductor Corp & Maxim Integrated Products Inc;\n",
      "\tDumping its document links:\n",
      "\t\thttps://www.sec.gov/Archives/edgar/data/0000743316/000109581101500299/0001095811-01-500299.txt\n",
      "\t\thttps://www.sec.gov/Archives/edgar/data/0000743316/000109581101500103/0001095811-01-500103.txt\n"
     ]
    }
   ],
   "source": [
    "for mainIndex in range(11,12):\n",
    "    print(\"Processing index: \", mainIndex, \"; Companies: \", companyAList[mainIndex], \" & \", companyBList[mainIndex]);\n",
    "\n",
    "    constraintDates = getDateConstraints(filedDate[mainIndex]);\n",
    "    lbDate, ubDate = constraintDates;\n",
    "    restructLB = f\"{lbDate.year}-{lbDate.month:02}-{lbDate.day:02}\";\n",
    "    restructUB = f\"{ubDate.year}-{ubDate.month:02}-{ubDate.day:02}\";\n",
    "    restructForms = \"%2C\".join(formTypes).replace(\" \", \"%20\");\n",
    "\n",
    "    # Find the documents with CIK filtering\n",
    "    results = getCIKDocumentJson(companyAList[mainIndex], companyBList[mainIndex], restructLB, restructUB, restructForms);\n",
    "    if (results == None): # Acquire all documents within our guess\n",
    "        results = getDocumentJson(companyAList[mainIndex], companyBList[mainIndex], restructLB, restructUB, restructForms);\n",
    "\n",
    "    # No documents found for our 2 companies\n",
    "    if (results == None):\n",
    "        logMessage(f\"[{get_current_timestamp()}] [-] No documents found for: {companyAList[mainIndex]} & {companyBList[mainIndex]}\");\n",
    "        continue;\n",
    "\n",
    "    # Extract the source document links\n",
    "    sourceLinks = getSourceLinks(results);\n",
    "\n",
    "    \"\"\"\n",
    "        - Here, we will verify that both company names are present in the document.\n",
    "            - Reduces the amount of documents needed to be processed with NLP.\n",
    "        - Next, if both company names are present, we will try and locate the \"Background of the Merger\"\n",
    "        chronological timeline.\n",
    "    \"\"\"\n",
    "    companyNames = [companyAList[mainIndex], companyBList[mainIndex]];\n",
    "    companyNames = [extractFirstWord(name).lower() for name in companyNames];\n",
    "\n",
    "    # Locate the documents with both company names present.\n",
    "    foundData = False;\n",
    "    terminationEvent.clear();\n",
    "    with ThreadPoolExecutor(max_workers=maxNumOfThreads) as executor:\n",
    "        futures = {executor.submit(checkCompaniesInDocument, url, companyNames): url for url in sourceLinks};\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            if (terminationEvent.is_set()):  # If background section is found already\n",
    "                break;\n",
    "\n",
    "            try:\n",
    "                cleanedText, bothFound = future.result();\n",
    "                print(bothFound);\n",
    "                if bothFound:\n",
    "                    # Additional preprocess cleaning\n",
    "                    cleanedText = removeTableOfContents(cleanedText);\n",
    "                    truncatedText = cleanedText[50000:1000000];  # Shrink to manageable size for spaCy\n",
    "                    print(truncatedText)\n",
    "\n",
    "                    backgroundSection = extractSection(truncatedText, startPhrases, stopPhrases);\n",
    "                    if backgroundSection is None:\n",
    "                        continue;\n",
    "                    \n",
    "                    # print(futures[future]);\n",
    "                    # print(backgroundSection);\n",
    "\n",
    "                    # Write the data to a file\n",
    "                    foundData = True;\n",
    "                    formatDocName = f\"{companyAList[mainIndex].replace(' ', '_')}_&_{companyBList[mainIndex].replace(' ', '_')}\";\n",
    "                    with open(f\"../DataSet/{formatDocName}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                        file.write(f\"URL: {futures[future]}\\n\\n\");\n",
    "                        file.write(backgroundSection);\n",
    "                    \n",
    "                    logMessage(f\"[{get_current_timestamp()}] [+] Successfully created document for: {companyAList[mainIndex]} & {companyBList[mainIndex]}\");\n",
    "\n",
    "                    # Signal termination and exit\n",
    "                    terminationEvent.set();\n",
    "                    break;\n",
    "            except Exception as e:\n",
    "                url = futures[future];\n",
    "                logMessage(f\"[{get_current_timestamp()}] [-] Error processing {url}: {e}\");\n",
    "\n",
    "    if not foundData:\n",
    "        logMessage(f\"[{get_current_timestamp()}] [-] No background section found for index {mainIndex}: {companyAList[mainIndex]} & {companyBList[mainIndex]};\");\n",
    "        logMessage(f\"\\tDumping its document links:\");\n",
    "        for url in sourceLinks:\n",
    "            logMessage(f\"\\t\\t{url}\");\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MergeExtractorEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
